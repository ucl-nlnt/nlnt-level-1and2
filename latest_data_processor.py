import os
import zlib
import json
import quart_funcs
import random
# from nlnt_types import StatesList
from transformers import AutoTokenizer
import math
import sys
from collections import deque
import shutil

precision = 3

def open_data_packet(path):

    with open(path, 'rb') as f:
        # Read the entire file content as bytes
        file_content = f.read()
    # Decompress the bytes-like object and then load it as JSON
    return json.loads(zlib.decompress(file_content))

def quaternion_to_yaw(x, y, z, w):  # Generated by GPT-4
    """
    Convert a quaternion into yaw (rotation around z-axis in radians)
    """
    t3 = +2.0 * (w * z + x * y)
    t4 = +1.0 - 2.0 * (y * y + z * z)
    yaw_z = math.atan2(t3, t4)
    return yaw_z

def find_keyframes_indexes(state_packet):

    frames = state_packet['states']


    #for i in range(len(frames)):

    twist_last = frames[0]['twist']
    keyframes = [0]
    #print(twist_last['linear'], twist_last['angular'])

    for i, frame in enumerate(frames):

        if frame['twist']['linear'] != twist_last['linear'] or frame['twist']['angular'] != twist_last['angular']:
            #print(frame['twist']['linear'], twist_last['linear'], frame['twist']['angular'], twist_last['angular'])
            twist_last = frame['twist']
            keyframes.append(i)

    return keyframes

def prep_frame_data(state_list: list, keyframe_indexes):

    data = [state_list[i] for i in keyframe_indexes]

    # shift and rotate odometry data
    reference_position = data[0]['odometry']['pose_position']
    reference_rotation = data[0]['odometry']['pose_orientation_quarternion']

    inverse_reference_rotation = quart_funcs.inverse_quarternion(
        reference_rotation)

    for i, frame in enumerate(data):

        odom = frame['odometry']
        old_position, old_rotation = odom['pose_position'], odom['pose_orientation_quarternion']

        new_position = quart_funcs.adjust_position_origin(
            reference_position, old_position)
        new_rotation = quart_funcs.adjust_orientation_origin(
            inverse_reference_rotation, old_rotation)

        odom['pose_position'] = new_position
        odom['pose_orientation_quarternion'] = new_rotation

        data[i]['odometry'] = odom

    return data # odometry data

def compute_distance(coord1, coord2):

    total = 0
    for i in range(3):
        total += (coord2[i] - coord1[i])**2

    return math.sqrt(total)

def phi3_format(chat_history: list):

    output = ""
    for i in chat_history:
        
        if i['role'] == 'user':
            output += '<|user|>' + '\n'

        elif i['role'] == 'assistant' or i['role'] == 'assist':
            output += '<|assistant|>' + '\n'

        output += i['content'].strip() + '<|end|>' + '\n'

    output = output.strip()

    return output

def split_into_train_and_eval(some_list: list):

    # uses 80-20 split
    random.shuffle(some_list)
    train_max = int(len(some_list) * 0.8)
    train = some_list[:train_max]
    eval_set = some_list[train_max:]

    return (train, eval_set)
    
training_entries_impossible = []
# open impossible and rephrased

import ast
file_paths_impossible = [os.path.join(os.getcwd(), 'impossible_rephrased', i) for i in os.listdir('impossible_rephrased/')]
skipped = 0

for path in file_paths_impossible:
    
    
    k = ""
    with open(path, 'r') as f:
        for j in f.readlines():
            k += j

    data = ast.literal_eval(k)

    if "### Possibility: True" in data['explanation']: 
        skipped += 1
        continue

    sys_prompt = f"""
I need you to break-down this natural language prompt into a series of steps to figure out whether or not it is possible to complete with a robot, the Turtlebot3.

The Turtlebot3 has the following sensors and capabilities:
1.) Moving around on the floor.
    - It cannot move even over small obstacles
    - It can move forwards and turn left or right, but it cannot be made to move backwards by its own. It needs to turn around first and then move backwards.
2.) Scan the immediate area around it with a planar lidar sensor, up to a maximum of 3.5 meters away from the sensor.
3.) Take a photo (maximum resolution of 1080 x 1920) or a video (720 x 1280 @ 30fps).
    - The Turtlebot3 can take a single photo or start a video stream.
    - Cannot take video in the dark.
    - Cannot be swiveled up and down, and is dependent on the robot to be rotated.
4.) Record audio.
    - Also supported while taking a video stream.
5.) Odometer
6.) IMU

The following device flags are available:

1.) take_photo : takes a photo
2.) livestream : sends a livestream of data frames to the server that the robot is connected to
3.) take_video : records a video of the activity
4.) audio_record : records audio
5.) audio : records audio data with either a video or a livestream; defaults to false so you need to indicate it if it needs to be used
6.) no_special_features : used to signal that the command doesn't need the robot to use any of its special sensors or features

The following actions are not supported:

1.) Object manipulation.
    - The Turtlebot3 does not have an arm or any of the sort to touch and move objects.

2.) Streaming media to and from the internet.
    - The Turtlebot3 does not have access to the internet

3.) Self-tilting or rolling on side
    - the robot is required to stay upright all the time

Add delimeters to allow a Python program to parse your answer afterwards, namely:
"<explanation_start>" and "<explanation_end>" to delimit your prompt breakdown,
"### Possibility: True" or "### Possibility: False" to denote prompt completionability,
and finally "<device_flags_start> and "<device_flags_end>" to enable/disable special sensors.

If the prompt is nonsensical, intentionally confusing, or seems to be just spam, flag it as ### Possibility: False

The natural language prompt that I want you to break-down is: {data['natural_language_prompt']}
"""

    chat = [
        {"role":"user","content" : sys_prompt},
        {"role":"assist", "content" : data['explanation'].strip() + "\n<explanation_end>"}]

    training_entries_impossible.append(phi3_format(chat))

impossible_train, impossible_eval = split_into_train_and_eval(training_entries_impossible)

print(len(impossible_train))
print(len(impossible_eval))

# handle actual possible prompts here
file_paths_possible = [os.path.join(os.getcwd(), 'training_data_pre_rephrased', i) for i in os.listdir('training_data_pre_rephrased/')]

train_split_possible_paths, eval_split_possible_paths = split_into_train_and_eval(file_paths_possible)

training_examples_breakdowns = []
training_examples_state_predictions = []

possible_skipped = 0
progress_possible = 1
for i in train_split_possible_paths[:2]:
    
    print(f"Progess:{progress_possible / len(train_split_possible_paths) * 100}")
    data = open_data_packet(i)
    
    progress_possible += 1
    if "### Possibility: False" in data['explanation']: 
        possible_skipped += 1
        continue

    sys_prompt = f"""
I need you to break-down this natural language prompt into a series of steps to figure out whether or not it is possible to complete with a robot, the Turtlebot3.

The Turtlebot3 has the following sensors and capabilities:
1.) Moving around on the floor.
    - It cannot move even over small obstacles
    - It can move forwards and turn left or right, but it cannot be made to move backwards by its own. It needs to turn around first and then move backwards.
2.) Scan the immediate area around it with a planar lidar sensor, up to a maximum of 3.5 meters away from the sensor.
3.) Take a photo (maximum resolution of 1080 x 1920) or a video (720 x 1280 @ 30fps).
    - The Turtlebot3 can take a single photo or start a video stream.
    - Cannot take video in the dark.
    - Cannot be swiveled up and down, and is dependent on the robot to be rotated.
4.) Record audio.
    - Also supported while taking a video stream.
5.) Odometer
6.) IMU

The following device flags are available:

1.) take_photo : takes a photo
2.) livestream : sends a livestream of data frames to the server that the robot is connected to
3.) take_video : records a video of the activity
4.) audio_record : records audio
5.) audio : records audio data with either a video or a livestream; defaults to false so you need to indicate it if it needs to be used
6.) no_special_features : used to signal that the command doesn't need the robot to use any of its special sensors or features

The following actions are not supported:

1.) Object manipulation.
    - The Turtlebot3 does not have an arm or any of the sort to touch and move objects.

2.) Streaming media to and from the internet.
    - The Turtlebot3 does not have access to the internet

3.) Self-tilting or rolling on side
    - the robot is required to stay upright all the time

Add delimeters to allow a Python program to parse your answer afterwards, namely:
"<explanation_start>" and "<explanation_end>" to delimit your prompt breakdown,
"### Possibility: True" or "### Possibility: False" to denote prompt completionability,
and finally "<device_flags_start> and "<device_flags_end>" to enable/disable special sensors.

If the prompt is nonsensical, intentionally confusing, or seems to be just spam, flag it as ### Possibility: False

The natural language prompt that I want you to break-down is: {data['natural_language_prompt']}
"""

    chat = [
        {"role":"user","content" : sys_prompt},
        {"role":"assist", "content" : data['explanation'].strip() + "\n<explanation_end>"}]

    
    training_examples_breakdowns.append(phi3_format(chat))
    #print(len(training_examples_breakdowns[-1]))

    # handle states here
    state_history = []
    keyframes = find_keyframes_indexes(data)
    prepped = prep_frame_data(data['states'], keyframes) # list of all keyframes

    skipped_frames = 0
    zero_dt = 0

    kstates_pruned = []
    for i in prepped:
        
        if i['twist']['linear'][0] == 0.0 and i['twist']['angular'][2] == 0.0:
            continue
        
        kstates_pruned.append(i)

    for i in range(len(prepped)):

        text = data['explanation'].replace("<explanation_start>","")
        
        current_frame = prepped[i - 1]
        next_frame = prepped[i]

        if [current_frame['twist']['linear'][0], current_frame['twist']['angular'][2]] == [0.0, 0.0]:
            
            skipped_frames += 1
            continue

        #print(hex(i - skipped_frames), hex(i - 1), i)
        twist_mess = [current_frame['twist']['linear'][0], current_frame['twist']['angular'][2]]
        instruction = "stop"
        execution_length = round(next_frame['twist']['time'] - current_frame['twist']['time'], precision)

        if execution_length == 0.0:
            print(zero_dt)
            zero_dt += 1
            continue

        if twist_mess[0] != 0.0:
            instruction = "move forwards"

        elif twist_mess[0] == 0.0 and twist_mess[1] > 0.0:
            instruction = "turn left"
        
        elif twist_mess[0] == 0.0 and twist_mess[1] < 0.0:
            instruction = "turn right"
        
        instruction_dict = {
            "total states" : len(kstates_pruned) - 1,
            "state number" : i - skipped_frames,
            "action" : instruction,
            "twist message" : [current_frame['twist']['linear'][0], current_frame['twist']['angular'][2]],
            "execution length" : round(next_frame['twist']['time'] - current_frame['twist']['time'], precision)
        }

        instruction_str = json.dumps(instruction_dict)

        if state_history != []:
            string_prompt = f"""
Given the following instruction breakdown, predict the next state. Use "<next_state_start>" and "<next_state_end>" to delineate your answer.

<prompt> {data['natural_language_prompt']} <prompt_end>
<state_history> {state_history} <state_history_end>

<breakdown>
{text}
<breakdown_end>
""".strip()
        else:
            string_prompt = f"""
Given the following instruction breakdown, predict the next state. Use "<next_state_start>" and "<next_state_end>" to delineate your answer.

<prompt_start> {data['natural_language_prompt']} <prompt_end>
<state_history_start> [None] <state_history_end>

<breakdown>
{text}
<breakdown_end>
""".strip()

        model_answer = f"""
The next state is:
<next_state_start>
{instruction_str}
<next_state_end>
""".strip()

        chat = [
        {"role":"user","content" : string_prompt},
        {"role":"assist", "content" : model_answer}]

        training_examples_state_predictions.append(phi3_format(chat))
        
        # print(action_examples[-1])
        #print(training_examples_breakdowns[-1])
        state_history.append(ast.literal_eval(instruction_str))
        if len(state_history) >= 6:
            state_history = state_history[1:]

    if instruction_dict['total states'] != instruction_dict['state number']:

        print(data['natural_language_prompt'])
        print('MISMATCHED LAST STATE WARNING', instruction_dict['total states'], instruction_dict['state number'])
        for debug in training_examples_state_predictions[len(training_examples_state_predictions) - instruction_dict['total states']:]:
            print('------')
            print(debug)

        sys.exit(0)




# handle actual possible prompts here

eval_examples_breakdowns = []
eval_examples_state_predictions = []

possible_skipped = 0
progress_possible = 1
for i in eval_split_possible_paths[:2]:

    print(f"Progess:{progress_possible / len(eval_split_possible_paths) * 100}")
    data = open_data_packet(i)
    
    progress_possible += 1
    if "### Possibility: False" in data['explanation']: 
        possible_skipped += 1
        continue

    sys_prompt = f"""
I need you to break-down this natural language prompt into a series of steps to figure out whether or not it is possible to complete with a robot, the Turtlebot3.

The Turtlebot3 has the following sensors and capabilities:
1.) Moving around on the floor.
    - It cannot move even over small obstacles
    - It can move forwards and turn left or right, but it cannot be made to move backwards by its own. It needs to turn around first and then move backwards.
2.) Scan the immediate area around it with a planar lidar sensor, up to a maximum of 3.5 meters away from the sensor.
3.) Take a photo (maximum resolution of 1080 x 1920) or a video (720 x 1280 @ 30fps).
    - The Turtlebot3 can take a single photo or start a video stream.
    - Cannot take video in the dark.
    - Cannot be swiveled up and down, and is dependent on the robot to be rotated.
4.) Record audio.
    - Also supported while taking a video stream.
5.) Odometer
6.) IMU

The following device flags are available:

1.) take_photo : takes a photo
2.) livestream : sends a livestream of data frames to the server that the robot is connected to
3.) take_video : records a video of the activity
4.) audio_record : records audio
5.) audio : records audio data with either a video or a livestream; defaults to false so you need to indicate it if it needs to be used
6.) no_special_features : used to signal that the command doesn't need the robot to use any of its special sensors or features

The following actions are not supported:

1.) Object manipulation.
    - The Turtlebot3 does not have an arm or any of the sort to touch and move objects.

2.) Streaming media to and from the internet.
    - The Turtlebot3 does not have access to the internet

3.) Self-tilting or rolling on side
    - the robot is required to stay upright all the time

Add delimeters to allow a Python program to parse your answer afterwards, namely:
"<explanation_start>" and "<explanation_end>" to delimit your prompt breakdown,
"### Possibility: True" or "### Possibility: False" to denote prompt completionability,
and finally "<device_flags_start> and "<device_flags_end>" to enable/disable special sensors.

If the prompt is nonsensical, intentionally confusing, or seems to be just spam, flag it as ### Possibility: False

The natural language prompt that I want you to break-down is: {data['natural_language_prompt']}
"""

    chat = [
        {"role":"user","content" : sys_prompt},
        {"role":"assist", "content" : data['explanation'].strip() + "\n<explanation_end>"}]

    
    eval_examples_breakdowns.append(phi3_format(chat))
    #print(len(training_examples_breakdowns[-1]))

    print(phi3_format(chat))
    sys.exit()
    # handle states here
    state_history = []
    keyframes = find_keyframes_indexes(data)
    prepped = prep_frame_data(data['states'], keyframes) # list of all keyframes

    skipped_frames = 0

    kstates_pruned = []
    for i in prepped:
        
        if i['twist']['linear'][0] == 0.0 and i['twist']['angular'][2] == 0.0:
            continue
        
        kstates_pruned.append(i)

    #print(data['natural_language_prompt'])
    for i, data_frame in enumerate(prepped[1:]):

        text = data['explanation'].replace("<explanation_start>","")
        
        current_frame = prepped[i - 1]
        next_frame = prepped[i]

        if [current_frame['twist']['linear'][0], current_frame['twist']['angular'][2]] == [0.0, 0.0]:
            
            skipped_frames += 1
            continue

        execution_length = round(next_frame['twist']['time'] - current_frame['twist']['time'], precision)
        if execution_length == 0.0:
            continue

        #print(hex(i - skipped_frames), hex(i - 1), i)
        twist_mess = [current_frame['twist']['linear'][0], current_frame['twist']['angular'][2]]
        instruction = "stop"

        if twist_mess[0] != 0.0:
            instruction = "move forwards"

        elif twist_mess[0] == 0.0 and twist_mess[1] > 0.0:
            instruction = "turn left"
        
        elif twist_mess[0] == 0.0 and twist_mess[1] < 0.0:
            instruction = "turn right"
        
        instruction_str = {
            "total states" : len(kstates_pruned) - 1,
            "state number" : i - skipped_frames,
            "action" : instruction,
            "twist message" : [current_frame['twist']['linear'][0], current_frame['twist']['angular'][2]],
            "execution length" : round(next_frame['twist']['time'] - current_frame['twist']['time'], precision)
        }

        #print(instruction_str)
        instruction_str = json.dumps(instruction_str)
        
        if state_history != []:
            string_prompt = f"""
Given the following instruction breakdown, predict the next state. Use "<next_state_start>" and "<next_state_end>" to delineate your answer.

<prompt> {data['natural_language_prompt']} <prompt_end>
<state_history> {state_history} <state_history_end>

<breakdown>
{text}
<breakdown_end>
""".strip()
        else:
            string_prompt = f"""
Given the following instruction breakdown, predict the next state. Use "<next_state_start>" and "<next_state_end>" to delineate your answer.

<prompt_start> {data['natural_language_prompt']} <prompt_end>
<state_history_start> [None] <state_history_end>

<breakdown>
{text}
<breakdown_end>
""".strip()

        model_answer = f"""
The next state is:
<next_state_start>
{instruction_str}
<next_state_end>
""".strip()

        chat = [
        {"role":"user","content" : string_prompt},
        {"role":"assist", "content" : model_answer}]

        eval_examples_state_predictions.append(phi3_format(chat))
        
        # print(action_examples[-1])
        #print(training_examples_breakdowns[-1])
        state_history.append(ast.literal_eval(instruction_str))
        if len(state_history) >= 6:
            state_history = state_history[1:]

        print(phi3_format(chat))
        sys.exit()
    if instruction_dict['total states'] != instruction_dict['state number']:

        
        print(data['natural_language_prompt'])
        print('MISMATCHED LAST STATE WARNING', instruction_dict['total states'], instruction_dict['state number'])
        for debug in training_examples_state_predictions[len(training_examples_state_predictions) - instruction_dict['total states']:]:
            print('------')
            print(debug)

        sys.exit(0)


impossible_train += training_examples_breakdowns
impossible_train += training_examples_state_predictions

random.shuffle(impossible_train)

impossible_eval += eval_examples_breakdowns
impossible_eval += eval_examples_state_predictions


#data = impossible_eval[0]
#data = data[data.rfind('<explanation_start'):data.rfind('<explanation_end>')]
#print(data)

with open(os.path.join("training_data_post","train","data.json"),'w') as f:

    f.write(json.dumps(impossible_train))

with open(os.path.join("training_data_post","eval","data.json"),'w') as f:

    f.write(json.dumps(impossible_eval))
    