import os
import zlib
import json
import quart_funcs
import random
# from nlnt_types import StatesList
from transformers import AutoTokenizer
import math
import sys
from collections import deque
import shutil
""" Data packet structure
    {
    username : ...,
    natural_language_prompt : ...,
    timestamp_s : ...,
    timestamp_float : ...,
    ground_truth : ...,
    simulation : int [optional],
    states : [
        {
            laser_scan : {
                None
                NOTE: currently unsupported in level 1 and 2 simulated data 
            },

            twist : {
                linear : [x, y, z] # usually x is the only non-zero
                angular : [x, y, z] # usually z is the only non-zero
                time : float
            },

            imu : {
                quarternion_orientation : [...],
                orientation_covariance : [...],
                angular_velocity : [x, y ,z],
                angular_velocity_covariance : [...],
                linear_acceleration : [...],
                linear_acceleration_covariance : [...]
            },

            odometry : {
                time_sec : float,
                time_nano : float,
                pose_position : [x, y, z],
                pose_orientation_quarternion : [x, y, z, w],
                object_covariance : float array [...] # usually not useful
            },

            battery : { # not usually useful
                ...
            },

            frame_data : np.ndarray # turtlebot 3 camera image; NOTE: None for simulated as of april 8, 2024
            distance_traveled : float,
            radians_rotated : float,

        },
        {
        ...
        },
        ...
    ]


    }
"""

def get_list_of_files(folder):

    p = os.getcwd()
    folder_path = os.path.join(p, folder)
    return [os.path.join(folder_path, i) for i in os.listdir(folder_path)]

def open_data_packet(path):

    with open(path, 'rb') as f:
        # Read the entire file content as bytes
        file_content = f.read()
    # Decompress the bytes-like object and then load it as JSON
    return json.loads(zlib.decompress(file_content))

def quaternion_to_yaw(x, y, z, w):  # Generated by GPT-4
    """
    Convert a quaternion into yaw (rotation around z-axis in radians)
    """
    t3 = +2.0 * (w * z + x * y)
    t4 = +1.0 - 2.0 * (y * y + z * z)
    yaw_z = math.atan2(t3, t4)
    return yaw_z

def find_keyframes_indexes(state_packet):

    frames = state_packet['states']

    twist_last = frames[0]['twist']
    keyframes = [0]
    for i, frame in enumerate(frames):
        if frame['twist'] != twist_last:
            twist_last = frame['twist']
            keyframes.append(i)

    return keyframes

def prep_frame_data(state_list: list, keyframe_indexes):

    data = [state_list[i] for i in keyframe_indexes]

    # shift and rotate odometry data
    reference_position = data[0]['odometry']['pose_position']
    reference_rotation = data[0]['odometry']['pose_orientation_quarternion']

    inverse_reference_rotation = quart_funcs.inverse_quarternion(
        reference_rotation)

    for i, frame in enumerate(data):

        odom = frame['odometry']
        old_position, old_rotation = odom['pose_position'], odom['pose_orientation_quarternion']

        new_position = quart_funcs.adjust_position_origin(
            reference_position, old_position)
        new_rotation = quart_funcs.adjust_orientation_origin(
            inverse_reference_rotation, old_rotation)

        odom['pose_position'] = new_position
        odom['pose_orientation_quarternion'] = new_rotation

        data[i]['odometry'] = odom

    return data

def compute_distance(coord1, coord2):

    total = 0
    for i in range(3):
        total += (coord2[i] - coord1[i])**2

    return math.sqrt(total)

def create_training_entry_from_packet(data_packet, keyframe_indexes, filename = None):

    # Gemma: {'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>'}
    # Mistral: // to be filled out later
    # Llama: // to be filled out later

    prompt = data_packet['natural_language_prompt']
    kstates = prep_frame_data(data_packet['states'], keyframe_indexes)
    training_examples = []
    
    entry_template = f"""You are given the task to act as a helpful agent that pilots a robot. Given the the frame history, determine the next frame in the series given the prompt and the previous state. Expect that any given data will be in the form of a JSON, and it is also expected that your reply will be also in JSON format. Set the 'completed' flag to '#complete' when you are done, otherwise leave it as '#ongoing'. Here is your task: {prompt}."""

    frame_history_list = deque([])

    precision = 3
    skipped_frames = 0

    for i in range(1, len(kstates)):
        
        current_frame = kstates[i-1]
        next_frame = kstates[i]

        try:

            twist_instruction = (current_frame['twist']['linear'][0], current_frame['twist']['angular'][2])

            if twist_instruction == (0.0, 0.0):
                skipped_frames += 1
                continue

            # experiment: turn all floating point values into integers
      
            frame = {

                "state number" : hex(i - 1 - skipped_frames),
                "orientation" : round(quaternion_to_yaw(*current_frame['odometry']['pose_orientation_quarternion']), precision),
                "distance to next point" : round(abs(compute_distance(current_frame['odometry']['pose_position'], next_frame['odometry']['pose_position'])),precision),
                "execution length" :  round(next_frame['twist']['time'] - current_frame['twist']['time'], precision),
                "movement message" : (current_frame['twist']['linear'][0], current_frame['twist']['angular'][2]),
                "instruction complete" : "#ongoing" if i != len(kstates)-1 else '#complete'

            }

        except Exception as e:
            print('Timeless twist message detected.')
            if filename != None:
                shutil.move(filename, os.path.join("twistless",filename.split('/')[-1]))
                print('Moved to another folder for now.')
            return [-1] # will be eliminated later

        if i == 1: # first iteration.
            entry = entry_template + ' | ' + 'History: [ None ] ' + "### Answer: " + str(frame) + ' <|endoftext|>'
            frame_history_list.append(frame)
            training_examples.append(entry)
            continue
        
        else:

            entry = entry_template + ' | ' + f'History: {list(frame_history_list)} ' + "### Answer: " + str(frame) +' <|endoftext|>'
            training_examples.append(entry)

        frame_history_list.append(frame)
        if len(frame_history_list) > 5: frame_history_list.popleft() # maximum history length
        
    return training_examples

def read_and_process_all(data_path, size, eval_size = -1):

    entries = []
    entries_validation = []

    paths = get_list_of_files(data_path)
    random.shuffle(paths)

    if size != -1:
        if eval_size == -1:
            validation = paths[size:]
        elif eval_size > 0:
            validation = paths[size:size+eval_size]
        else:
            validation = []

    print("Creating training prompts...")
    for i, path in enumerate(paths[:size]): # training
        if i % 100 == 0: print(i/size * 100)
        packet = open_data_packet(path)
        training_entries = create_training_entry_from_packet(
            packet, find_keyframes_indexes(packet), path)
        entries += training_entries

    if eval_size != 0:
        print("Creating eval prompts...")
        if eval_size == -1: ev_size = len(paths) - size
        else: ev_size = eval_size

        for i, path in enumerate(validation): # validation
            if i % 100 == 0: print(i/ev_size * 100)
            packet = open_data_packet(path)
            training_entries = create_training_entry_from_packet(
                packet, find_keyframes_indexes(packet), path
            )
            entries_validation += training_entries
    
    print("training data entries:", len(entries))
    print("validation entries:", len(entries_validation))
    return (entries, entries_validation, paths, validation)

tokenizer_name = "mistralai/Mistral-7B-Instruct-v0.2"
tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)

print(tokenizer.special_tokens_map.keys())

entries, eval_entries, train_paths, eval_paths = read_and_process_all('training_data_pre', size=21100, eval_size=-1)

# for i in entries: # debug
#    print(i)
#    print('\n')

with open('train_paths.txt','w') as f:
    for i in train_paths:
        f.write(i + '\n')

with open('eval_paths.txt','w') as f:
    for i in eval_paths:
        f.write(i + '\n')

errors_popped = 0
while -1 in entries:
    entries.pop(entries.index(-1))
    print('popped an error, [train]')
    errors_popped += 1

while -1 in eval_entries:
    eval_entries.pop(eval_entries.index(-1))
    print('popped an error, [eval]')
    errors_popped += 1

print("Errors removed:", errors_popped)

print("size [train]:",len(entries))
entries = list(set(entries))
print("size [eval]:",len(eval_entries))
eval_entries = list(set(eval_entries))

print("Size with no repetitions [train]:", len(entries))
print("Size with no repetitions [eval]:", len(eval_entries))
random.shuffle(entries)
random.shuffle(eval_entries)

print("test:", tokenizer.tokenize('<s> </s>'))
# print(entries[0])

# Replace 'your_dataset_name' with the actual name of your dataset
# and 'your_dataset_field' with the field name containing the text you want to tokenize

# Initialize the tokenizer
# Replace 'your_model_name' with the actual model name you're using, e.g., "bert-base-uncased"


# Function to calculate the number of tokens in each entry

def count_tokens(example):
    return len(tokenizer.tokenize(example))

tokens = []
for i, j in enumerate(entries):
    tokens.append(count_tokens(j))

tokens_eval = []
for i, j in enumerate(eval_entries):
    tokens_eval.append(count_tokens(j))

# print(entries[tokens.index(max(tokens))])

# Apply the function across the dataset to find the max number of tokens
# Note: Depending on the size of your dataset, this operation might take some time.
max_tokens = max(tokens)
max_tokens_eval = max(tokens_eval) if len(eval_entries) > 0 else 0
print("Maximum tokens train:", max_tokens)
print("Maximum tokens eval:", max_tokens_eval)

with open(os.path.join('training_data_post', 'train', 'data.json'), 'w') as f:
    f.write(json.dumps(entries))

with open(os.path.join('training_data_post', 'eval', 'data.json'), 'w') as f:
    f.write(json.dumps(eval_entries))
