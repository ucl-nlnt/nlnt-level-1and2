import os
import zlib
import json
import quart_funcs
import random
<<<<<<< HEAD
from custom_types import DatalogType, StatesList
from transformers import AutoTokenizer
=======
import math
>>>>>>> daa10c480d86ea0bce0bfdbe6b9df3dd590151a8

""" Data packet structure
    {
    username : ...,
    natural_language_prompt : ...,
    timestamp_s : ...,
    timestamp_float : ...,
    ground_truth : ...,
    simulation : int [optional],
    states : [
        {
            laser_scan : {
                None
                NOTE: currently unsupported in level 1 and 2 simulated data 
            },

            twist : {
                linear : [x, y, z] # usually x is the only non-zero
                angular : [x, y, z] # usually z is the only non-zero
            },

            imu : {
                quarternion_orientation : [...],
                orientation_covariance : [...],
                angular_velocity : [x, y ,z],
                angular_velocity_covariance : [...],
                linear_acceleration : [...],
                linear_acceleration_covariance : [...]
            },

            odometry : {
                time_sec : float,
                time_nano : float,
                pose_position : [x, y, z],
                pose_orientation_quarternion : [x, y, z, w],
                object_covariance : float array [...] # usually not useful
            },

            battery : { # not usually useful
                ...
            },

            frame_data : np.ndarray # turtlebot 3 camera image; NOTE: None for simulated as of april 8, 2024
            distance_traveled : float,
            radians_rotated : float,

        },
        {
        ...
        },
        ...
    ]


    }
"""


def get_list_of_files(folder):

    p = os.getcwd()
    folder_path = os.path.join(p, folder)
    return [os.path.join(folder_path, i) for i in os.listdir(folder_path)]


def open_data_packet(path):

    with open(path, 'rb') as f:
        # Read the entire file content as bytes
        file_content = f.read()
    # Decompress the bytes-like object and then load it as JSON
    return json.loads(zlib.decompress(file_content))

<<<<<<< HEAD

def find_keyframes_indexes(state_packet: DatalogType):
=======
def quaternion_to_yaw(x, y, z, w): # Generated by GPT-4

    """
    Convert a quaternion into yaw (rotation around z-axis in radians)
    """
    t3 = +2.0 * (w * z + x * y)
    t4 = +1.0 - 2.0 * (y * y + z * z)
    yaw_z = math.atan2(t3, t4)
    return yaw_z

def find_keyframes_indexes(state_packet):
>>>>>>> daa10c480d86ea0bce0bfdbe6b9df3dd590151a8

    frames = state_packet['states']

    twist_last = frames[0]['twist']
    keyframes = [0]
    for i, frame in enumerate(frames):
        if frame['twist'] != twist_last:
            twist_last = frame['twist']
            keyframes.append(i)

    return keyframes


def prep_frame_data(state_list: StatesList, keyframe_indexes):

    data: StatesList = [state_list[i] for i in keyframe_indexes]

    # shift and rotate odometry data

    reference_position = data[0]['odometry']['pose_position']
    reference_rotation = data[0]['odometry']['pose_orientation_quarternion']

    inverse_reference_rotation = quart_funcs.inverse_quarternion(
        reference_rotation)

    for i, frame in enumerate(data):

        odom = frame['odometry']
        old_position, old_rotation = odom['pose_position'], odom['pose_orientation_quarternion']

        new_position = quart_funcs.adjust_position_origin(
            reference_position, old_position)
        new_rotation = quart_funcs.adjust_orientation_origin(
            inverse_reference_rotation, old_rotation)

        odom['pose_position'] = new_position
        odom['pose_orientation_quarternion'] = new_rotation

        data[i]['odometry'] = odom

    return data

<<<<<<< HEAD

def create_training_entry_from_packet(data_packet: DatalogType, keyframe_indexes, bos='<bos>', eos='<eos>'):
=======
def compute_distance(coord1, coord2):

    total = 0
    for i in range(3):
        total += (coord2[i] - coord1[i])**2
    
    return math.sqrt(total)

def create_training_entry_from_packet(data_packet, keyframe_indexes, bos = '<bos>', eos = '<eos>'):
>>>>>>> daa10c480d86ea0bce0bfdbe6b9df3dd590151a8

    # Gemma: {'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>'}
    # Mistral: // to be filled out later
    # Llama: // to be filled out later

    prompt = data_packet['natural_language_prompt']
    kstates = prep_frame_data(data_packet['states'], keyframe_indexes)
    print(prompt)
    training_examples = []

    entry = f"{bos} [cmd] {prompt} [cmd_end]"
    state_number = 0
<<<<<<< HEAD
    for i in range(1, len(kstates)):

=======

    # remember to normalize data
    reference_coords = kstates[0]['odometry']['pose_position']
    reference_inverse_radial = quart_funcs.inverse_quarternion(kstates[0]['odometry']['pose_orientation_quarternion']) # needed to normalize radial readings

    for i in range(1,len(kstates)):
        
>>>>>>> daa10c480d86ea0bce0bfdbe6b9df3dd590151a8
        entry += ' [PRED] '

        current_kframe = kstates[i - 1]
        next_kframe = kstates[i]

        # I try to not use words that are present in the natural language dataset for the state keys
<<<<<<< HEAD

        dt = (keyframe_indexes[i] - keyframe_indexes[i-1]) * 0.1
        print(dt)
=======
        mess = [current_kframe['twist']['linear'][0], current_kframe['twist']['angular'][2]]

        precision = 2

        try:
            dt = round(next_kframe['twist']['time'] - current_kframe['twist']['time'], precision)

        except:

            print("Error encountered processing.")
            return [-1]

>>>>>>> daa10c480d86ea0bce0bfdbe6b9df3dd590151a8
        entry += str(

            {
<<<<<<< HEAD
                'state number': state_number,
                'message': current_kframe['twist'],
                'coordinates': [round(i, 2) for i in current_kframe['odometry']['pose_position']],
                'radial': [round(i, 2) for i in current_kframe['odometry']['pose_orientation_quarternion']],
                'dt': dt,
                'completed': '#ONGOING' if i+1 != len(kstates) else "#COMPLETE"
=======
                'state number' : state_number,
                'message' : [current_kframe['twist']['linear'][0], current_kframe['twist']['angular'][2]] ,
                'xdist' : round(compute_distance(next_kframe['odometry']['pose_position'], current_kframe['odometry']['pose_position']),precision),
                'radial' : round(quaternion_to_yaw(*quart_funcs.adjust_orientation_origin(reference_inverse_radial,current_kframe['odometry']['pose_orientation_quarternion'])), precision),
                'dt' : dt,
                'completed' : '#ONGOING' if i+1 != len(kstates) else "#COMPLETE"
>>>>>>> daa10c480d86ea0bce0bfdbe6b9df3dd590151a8
            }

        )

        state_number += 1
<<<<<<< HEAD
        training_examples.append(entry + ' ' + eos)  # add to training examples

        if i+1 != len(kstates):
            entry = entry.replace('[PRED]', '[OBS]')
        break
=======
        training_examples.append(entry + ' ' + eos) # add to training examples
        
        if i+1 != len(kstates): entry = entry.replace('[PRED]', '[OBS]')
        
>>>>>>> daa10c480d86ea0bce0bfdbe6b9df3dd590151a8
    return training_examples


def read_and_process_all(data_path, size, bos, eos):

    entries = []

    paths = get_list_of_files(data_path)
    random.shuffle(paths)
    if size != -1:
        paths = paths[:size]

    for path in paths:
        packet = open_data_packet(path)
        training_entries = create_training_entry_from_packet(
            packet, find_keyframes_indexes(packet), bos=bos, eos=eos)
        entries += training_entries

    print("number of training data entries:", len(entries))
    return entries

<<<<<<< HEAD

entries = read_and_process_all(
    'training_data_pre', size=3000, bos="<s>", eos="</s>")
=======
entries = read_and_process_all('training_data_pre', size=3000, bos="<s>", eos="</s>")
while -1 in entries: entries.pop(entries.index(-1)); print('popped an error')
>>>>>>> daa10c480d86ea0bce0bfdbe6b9df3dd590151a8
print(entries[-1])
print(len(entries))
random.shuffle(entries)
# print(entries[0])


# Replace 'your_dataset_name' with the actual name of your dataset
# and 'your_dataset_field' with the field name containing the text you want to tokenize

# Initialize the tokenizer
# Replace 'your_model_name' with the actual model name you're using, e.g., "bert-base-uncased"
tokenizer_name = "mistralai/Mistral-7B-Instruct-v0.2"
tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)

# Function to calculate the number of tokens in each entry


def count_tokens(example):
    return len(tokenizer.tokenize(example))

<<<<<<< HEAD

tokens = [count_tokens(i) for i in entries]
=======
tokens = []
for i, j in enumerate(entries):
    tokens.append(count_tokens(j))

print(max(tokens))
print(entries[tokens.index(max(tokens))])
>>>>>>> daa10c480d86ea0bce0bfdbe6b9df3dd590151a8

# Apply the function across the dataset to find the max number of tokens
# Note: Depending on the size of your dataset, this operation might take some time.
max_tokens = max(tokens)
print("Maximum tokens in a dataset entry:", max_tokens)

with open(os.path.join('training_data_post', 'train', 'data.json'), 'w') as f:
    f.write(json.dumps(entries))
